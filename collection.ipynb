{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74e5616d",
   "metadata": {},
   "source": [
    "# Solar System RAG: Advanced Pipeline & Comparison\n",
    "\n",
    "**Goal:** Build and compare two RAG pipelines (Local vs. OpenAI) for the Solar System.\n",
    "\n",
    "**Structure:**\n",
    "1. **Part 1: Python Cheat Sheet** - With detailed comments explaining the operations.\n",
    "2. **Part 2: Exam Simulation** - 15 Exercises to build the pipeline, including A/B testing and \"I don't know\" constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2fc87cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports from your course materials\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf82a2",
   "metadata": {},
   "source": [
    "## Part 1: Python Operations Cheat Sheet (Commented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76743931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. FILE & DIRECTORY MANAGEMENT ---\n",
    "# os.makedirs ensures the folder exists. 'exist_ok=True' prevents errors if it's already there.\n",
    "folder_name = \"test_folder\"\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# glob.glob is a pattern matcher. '*.txt' finds any file ending in .txt.\n",
    "# It returns a list of file paths like ['test_folder/a.txt', 'test_folder/b.txt']\n",
    "found_files = glob.glob(f\"{folder_name}/*.txt\")\n",
    "\n",
    "# --- 2. STRING MANIPULATION ---\n",
    "# We often need to turn a list of text chunks into one big string for the LLM.\n",
    "# '\\n---\\n'.join(list) puts a separator between each item.\n",
    "chunks = [\"Fact A\", \"Fact B\"]\n",
    "context_string = \"\\n---\\n\".join(chunks)\n",
    "# Result: \"Fact A\\n---\\nFact B\"\n",
    "\n",
    "# --- 3. LIST COMPREHENSION (The 'Pythonic' Loop) ---\n",
    "# Instead of writing a 3-line for-loop, we do it in one line.\n",
    "prices = [10, 20, 30]\n",
    "# Syntax: [OPERATION for ITEM in LIST]\n",
    "doubled = [p * 2 for p in prices]\n",
    "# Result: [20, 40, 60]\n",
    "\n",
    "# --- 4. DICTIONARY SAFE ACCESS ---\n",
    "# config['key'] crashes if the key is missing.\n",
    "# config.get('key', default) returns the default value instead.\n",
    "config = {\"model\": \"gpt-4\"}\n",
    "temp = config.get(\"temperature\", 0.7) # Returns 0.7 because 'temperature' isn't in config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d29843",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875a9e44",
   "metadata": {},
   "source": [
    "## Part 2: Solar System Pipeline Exercises\n",
    "\n",
    "We will build **two** configurations to compare them later:\n",
    "* **Config A:** Small chunks (100 chars), Local Embeddings (SentenceTransformer).\n",
    "* **Config B:** Large chunks (500 chars), OpenAI Embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a060a947",
   "metadata": {},
   "source": [
    "### Exercise 1: Setup & Directory\n",
    "**Task:** Initialize OpenAI and create `data_collection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4e8b6ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Create the folder to store our text/pdf files\n",
    "os.makedirs(\"data_collection\", exist_ok=True)\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321bc1ea",
   "metadata": {},
   "source": [
    "### Exercise 2: Basic Chat\n",
    "**Task:** Verify LLM connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e204f85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupiter.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the largest planet? Answer in 1 word.\"}]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b249a1d",
   "metadata": {},
   "source": [
    "### Exercise 3: Generate Solar System Data\n",
    "**Task:** Create 4 text files in `data_collection`. These are our \"knowledge base\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "be2bdbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge base created.\n"
     ]
    }
   ],
   "source": [
    "facts = {\n",
    "    \"mars.txt\": \"Mars is the Red Planet. It has two small moons, Phobos and Deimos. It is cold and dusty.\",\n",
    "    \"jupiter.txt\": \"Jupiter is the largest planet. It is a gas giant with a Great Red Spot storm. It has over 79 moons.\",\n",
    "    \"saturn.txt\": \"Saturn is known for its beautiful ring system made of ice and rock. Titan is its largest moon.\",\n",
    "    \"venus.txt\": \"Venus is the hottest planet (462°C) due to a thick CO2 atmosphere. It rotates backwards.\"\n",
    "}\n",
    "\n",
    "for name, content in facts.items():\n",
    "    # os.path.join handles / vs \\ separators automatically for Windows/Mac\n",
    "    path = os.path.join(\"data_collection\", name)\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"Knowledge base created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1113bcb2",
   "metadata": {},
   "source": [
    "### Exercise 4: Universal Loader (TXT & PDF)\n",
    "**Task:** Write a function that reads **both** `.txt` and `.pdf` files from the directory into a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "89669ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 14078 characters.\n"
     ]
    }
   ],
   "source": [
    "def load_data(folder):\n",
    "    combined_text = \"\"\n",
    "    \n",
    "    # 1. Grab all TXT files\n",
    "    # glob returns a list of paths\n",
    "    for filepath in glob.glob(os.path.join(folder, \"*.txt\")):\n",
    "        with open(filepath, \"r\") as f:\n",
    "            combined_text += f.read() + \"\\n\"\n",
    "\n",
    "    # 2. Grab all PDF files\n",
    "    # We use PyPDF2 to parse the binary PDF format\n",
    "    for filepath in glob.glob(os.path.join(folder, \"*.pdf\")):\n",
    "        try:\n",
    "            reader = PdfReader(filepath)\n",
    "            for page in reader.pages:\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    combined_text += text + \"\\n\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filepath}: {e}\")\n",
    "            \n",
    "    return combined_text\n",
    "\n",
    "raw_text = load_data(\"data_collection\")\n",
    "print(f\"Loaded {len(raw_text)} characters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef33d714",
   "metadata": {},
   "source": [
    "### Exercise 5: Creating Config A (Local Pipeline)\n",
    "**Task:** Split text into `chunks_a` (Size: 100, Overlap: 20) and create embeddings using `SentenceTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "372dd19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config A] Chunks: 203 | Embedding Shape: (203, 384)\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIG A: Small chunks, Local Model ---\n",
    "splitter_a = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "chunks_a = splitter_a.split_text(raw_text)\n",
    "\n",
    "# Initialize local model (runs on CPU/GPU)\n",
    "embedder_a = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Create embeddings\n",
    "# convert_to_numpy=True is required for FAISS\n",
    "embeddings_a = embedder_a.encode(chunks_a, convert_to_numpy=True)\n",
    "\n",
    "print(f\"[Config A] Chunks: {len(chunks_a)} | Embedding Shape: {embeddings_a.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0d9777",
   "metadata": {},
   "source": [
    "### Exercise 6: Creating Config B (OpenAI Pipeline)\n",
    "**Task:** Split text into `chunks_b` (Size: 300, Overlap: 50) and create embeddings using `client.embeddings.create`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9aa2c386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config B] Chunks: 60 | Embedding Shape: (60, 1536)\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIG B: Larger chunks, OpenAI Model ---\n",
    "splitter_b = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks_b = splitter_b.split_text(raw_text)\n",
    "\n",
    "# OpenAI embeddings batch call\n",
    "# We send the list of strings (chunks_b) to the API\n",
    "response = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=chunks_b\n",
    ")\n",
    "\n",
    "# Extract the vector list from the response object\n",
    "vecs = [item.embedding for item in response.data]\n",
    "# Convert list of lists to NumPy array (float32 is standard for FAISS)\n",
    "embeddings_b = np.array(vecs, dtype=\"float32\")\n",
    "\n",
    "print(f\"[Config B] Chunks: {len(chunks_b)} | Embedding Shape: {embeddings_b.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92295664",
   "metadata": {},
   "source": [
    "### Exercise 7: Saving FAISS Indices (Persistence)\n",
    "**Task:** Create and save two separate FAISS indices: `index_a.index` and `index_b.index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "40cdb48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both indices saved to disk.\n"
     ]
    }
   ],
   "source": [
    "# Index A (Local Dim: 384)\n",
    "dim_a = embeddings_a.shape[1]\n",
    "index_a = faiss.IndexFlatL2(dim_a)\n",
    "index_a.add(embeddings_a)\n",
    "faiss.write_index(index_a, os.path.join(\"data_collection\", \"index_a.index\"))\n",
    "\n",
    "# Index B (OpenAI Dim: 1536 usually)\n",
    "dim_b = embeddings_b.shape[1]\n",
    "index_b = faiss.IndexFlatL2(dim_b)\n",
    "index_b.add(embeddings_b)\n",
    "faiss.write_index(index_b, os.path.join(\"data_collection\", \"index_b.index\"))\n",
    "\n",
    "print(\"Both indices saved to disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a234313",
   "metadata": {},
   "source": [
    "### Exercise 8: Saving Text Chunks\n",
    "**Task:** Save both chunk lists to pickle files so we can retrieve the text later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a3f69a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk text lists saved.\n"
     ]
    }
   ],
   "source": [
    "# We need to save the text because FAISS only stores numbers.\n",
    "with open(os.path.join(\"data_collection\", \"chunks_a.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(chunks_a, f)\n",
    "\n",
    "with open(os.path.join(\"data_collection\", \"chunks_b.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(chunks_b, f)\n",
    "\n",
    "print(\"Chunk text lists saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710a2431",
   "metadata": {},
   "source": [
    "### Exercise 9: Loading Everything Back\n",
    "**Task:** Load indices and chunks into variables to mimic a production server starting up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4d3bfa74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data loaded into memory.\n"
     ]
    }
   ],
   "source": [
    "# Load Indices\n",
    "loaded_idx_a = faiss.read_index(os.path.join(\"data_collection\", \"index_a.index\"))\n",
    "loaded_idx_b = faiss.read_index(os.path.join(\"data_collection\", \"index_b.index\"))\n",
    "\n",
    "# Load Text\n",
    "with open(os.path.join(\"data_collection\", \"chunks_a.pkl\"), \"rb\") as f:\n",
    "    loaded_chunks_a = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(\"data_collection\", \"chunks_b.pkl\"), \"rb\") as f:\n",
    "    loaded_chunks_b = pickle.load(f)\n",
    "\n",
    "print(\"All data loaded into memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a29a7f",
   "metadata": {},
   "source": [
    "### Exercise 10: Retrieval Function A (Local)\n",
    "**Task:** Implement `retrieve_a(query)` using `embedder_a` and `loaded_idx_a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ae179307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Venus is the hottest planet (462°C) due to a thick CO2 atmosphere. It rotates backwards.', 'heit). Because the planet has no atmosphere to retain that heat,']\n"
     ]
    }
   ],
   "source": [
    "def retrieve_a(query, k=2):\n",
    "    # 1. Embed query with Local model\n",
    "    q_vec = embedder_a.encode([query], convert_to_numpy=True)\n",
    "    \n",
    "    # 2. Search Index A\n",
    "    # search returns (distances, indices)\n",
    "    _, indices = loaded_idx_a.search(q_vec, k)\n",
    "    \n",
    "    # 3. Retrieve Text from Chunks A\n",
    "    return [loaded_chunks_a[i] for i in indices[0]]\n",
    "\n",
    "print(retrieve_a(\"Is Venus hot?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b05e6f",
   "metadata": {},
   "source": [
    "### Exercise 11: Retrieval Function B (OpenAI)\n",
    "**Task:** Implement `retrieve_b(query)` using OpenAI embeddings and `loaded_idx_b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ee3e06f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Venus is the hottest planet (462°C) due to a thick CO2 atmosphere. It rotates backwards.\\nSaturn is known for its beautiful ring system made of ice and rock. Titan is its largest moon.\\nMars is the Red Planet. It has two small moons, Phobos and Deimos. It is cold and dusty.', 'heit). Because the planet has no atmosphere to retain that heat, \\nnighttime temperatures on the surface can drop to –180  degrees \\nCelsius (–290 degrees Fahrenheit).\\nBecause Mercury is so close to the Sun, it is hard to directly ob -\\nserve from Earth except during dawn or twilight. Mercury makes']\n"
     ]
    }
   ],
   "source": [
    "def retrieve_b(query, k=2):\n",
    "    # 1. Embed query with OpenAI\n",
    "    resp = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=[query]\n",
    "    )\n",
    "    # Convert to numpy array, reshape to (1, dimension)\n",
    "    q_vec = np.array([resp.data[0].embedding], dtype=\"float32\")\n",
    "    \n",
    "    # 2. Search Index B\n",
    "    _, indices = loaded_idx_b.search(q_vec, k)\n",
    "    \n",
    "    # 3. Retrieve Text from Chunks B\n",
    "    return [loaded_chunks_b[i] for i in indices[0]]\n",
    "\n",
    "print(retrieve_b(\"Is Venus hot?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb8396",
   "metadata": {},
   "source": [
    "### Exercise 12: Advanced Prompting (Constraint)\n",
    "**Task:** Create `format_prompt` that includes strict instructions: **\"If the answer is not in the context, state 'I do not know'.\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e193f978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(query, docs):\n",
    "    context = \"\\n---\\n\".join(docs)\n",
    "    return f\"\"\"You are a strict assistant. Answer ONLY based on the context below.\n",
    "If the answer is not in the context, say 'I do not know'. Do not make things up.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944da678",
   "metadata": {},
   "source": [
    "### Exercise 13: RAG Answer Generators\n",
    "**Task:** Create `rag_answer_a(query)` and `rag_answer_b(query)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6d9b4792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer_a(query):\n",
    "    docs = retrieve_a(query)\n",
    "    prompt = format_prompt(query, docs)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def rag_answer_b(query):\n",
    "    docs = retrieve_b(query)\n",
    "    prompt = format_prompt(query, docs)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95b901b",
   "metadata": {},
   "source": [
    "### Exercise 14: Automated Question Generation\n",
    "**Task:** Generate a question from a random chunk in List A to test our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ad37ec0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Inclination of Equator to Orbit  29.58 deg  \n",
      "Rotation Period  16.11 hours\n",
      "Gen Question: What is the inclination of the equator to its orbit?\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "chunk = random.choice(loaded_chunks_a)\n",
    "print(f\"Source: {chunk}\")\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": f\"Write a short question for this text: {chunk}\"}]\n",
    ")\n",
    "generated_q = resp.choices[0].message.content\n",
    "print(f\"Gen Question: {generated_q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b02312",
   "metadata": {},
   "source": [
    "### Exercise 15: Side-by-Side Comparison & Out-of-Context Test\n",
    "**Task:** \n",
    "1. Define a list containing valid questions AND an **out-of-context** question (e.g., about France).\n",
    "2. Loop through them, getting answers from Pipeline A and Pipeline B.\n",
    "3. Display a Pandas DataFrame comparing the two results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e11525e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer A (Local)</th>\n",
       "      <th>Answer B (OpenAI)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the great red spot on jupiter?</td>\n",
       "      <td>The Great Red Spot is a storm on Jupiter.</td>\n",
       "      <td>The Great Red Spot is a storm on Jupiter.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How hot is Venus?</td>\n",
       "      <td>Venus is 462°C.</td>\n",
       "      <td>Venus is 462°C.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>I do not know.</td>\n",
       "      <td>I do not know.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Question  \\\n",
       "0  What is the great red spot on jupiter?   \n",
       "1                       How hot is Venus?   \n",
       "2          What is the capital of France?   \n",
       "\n",
       "                            Answer A (Local)  \\\n",
       "0  The Great Red Spot is a storm on Jupiter.   \n",
       "1                            Venus is 462°C.   \n",
       "2                             I do not know.   \n",
       "\n",
       "                           Answer B (OpenAI)  \n",
       "0  The Great Red Spot is a storm on Jupiter.  \n",
       "1                            Venus is 462°C.  \n",
       "2                             I do not know.  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_questions = [\n",
    "    \"What is the great red spot on jupiter?\",     # Should retrieve from Jupiter\n",
    "    \"How hot is Venus?\",                          # Should retrieve from Venus\n",
    "    \"What is the capital of France?\"              # Out of context -> Should say \"I do not know\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for q in test_questions:\n",
    "    # Run both pipelines\n",
    "    ans_a = rag_answer_a(q)\n",
    "    ans_b = rag_answer_b(q)\n",
    "    \n",
    "    results.append({\n",
    "        \"Question\": q,\n",
    "        \"Answer A (Local)\": ans_a,\n",
    "        \"Answer B (OpenAI)\": ans_b\n",
    "    })\n",
    "\n",
    "# Create Comparison Table\n",
    "df_compare = pd.DataFrame(results)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df_compare"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
