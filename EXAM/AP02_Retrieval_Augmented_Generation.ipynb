{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1845a46",
   "metadata": {},
   "source": [
    "# Arbeitspaket (AP) 2: Retrieval Augmented Generation (RAG)\n",
    "\n",
    "**Datenthema:** Jugend, Politik, Umwelt & Partizipation\n",
    "  \n",
    "**Maximale Punktzahl:** 60 Punkte\n",
    "\n",
    "---\n",
    "## Hinweise\n",
    "- Beantworten Sie alle Aufgaben direkt in diesem Notebook.\n",
    "- Ändern Sie Zellen mit dem Hinweis **\"Bitte nicht verändern\"** nicht.\n",
    "- Verwenden Sie, wo möglich, die bereits gegebenen Variablen und Funktionen.\n",
    "- Antworten auf Theoriefragen schreiben Sie in Markdown-Zellen.\n",
    "\n",
    "Die Daten für den RAG-Teil (Texte zu *Jugend, Politik, Umwelt & Partizipation* in deutscher Sprache) müsssen im Ordner `data/` abgelegt werden. Sie benötigen ebenfalls eine `.env`-Datei für die Keys.\n",
    "\n",
    "---\n",
    "## Teil A – LLM API-Aufrufe (25 Punkte)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5f44ee",
   "metadata": {},
   "source": [
    "### Persönliche Angaben (bitte ergänzen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb046ae0",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <td>Vorname:</td>\n",
    "    <td>Colin</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Nachname:</td>\n",
    "    <td>Zemp</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Immatrikulationsnummer:</td>\n",
    "    <td>17679390</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Modul:</td>\n",
    "    <td>Data Science</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Prüfungsdatum / Raum / Zeit:</td>\n",
    "    <td>15.12.2025 / Raum: MU O2.001 / 8:00 – 11:45</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Erlaubte Hilfsmittel:</td>\n",
    "    <td>w.MA.XX.DS.24HS (Data Science)<br>Open Book, Eigener Computer, Internet-Zugang</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td>Nicht erlaubt:</td>\n",
    "  <td>Nicht erlaubt ist der Einsatz beliebiger Formen von generativer KI (z.B. Copilot, ChatGPT) <br> sowie beliebige Formen von Kommunikation oder Kollaboration mit anderen Menschen.</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c395fab0",
   "metadata": {},
   "source": [
    "## Bewertungskriterien\n",
    "\n",
    "| **Kategorie**                       | **Beschreibung**                                                                                                                                          | **Punkteverteilung**                |\n",
    "|-------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------|\n",
    "| **Code nicht lauffähig oder Ergebnisse nicht relevant** | Der Code läuft nicht oder erfüllt nicht die Anforderungen des Aspekts (z. B. Bilder werden nicht geladen, die Textausgabe der Extraktion fehlt, Bounding Boxes werden nicht angezeigt). | **0 Punkte**                       |\n",
    "| **Code lauffähig, aber mit gravierenden Mängeln**       | Der Code läuft, jedoch fehlen zentrale Teile der Funktionalität eines Aspekts (z. B. unvollständige Extraktion von Bildinformationen oder Fehler bei der Definition eines Schemas). | **25% der max. erreichbaren Punkte** |\n",
    "| **Code lauffähig, aber mit mittleren Mängeln**          | Der Code läuft und liefert teilweise korrekte Ergebnisse für einen Aspekt, aber wichtige Details fehlen (z. B. ungenaue Bounding Boxes, unvollständige Integration der extrahierten Daten). | **50% der max. erreichbaren Punkte** |\n",
    "| **Code lauffähig, aber mit minimalen Mängeln**          | Der Code erfüllt die Anforderungen eines Aspekts weitgehend, aber kleinere Fehler oder Abweichungen (z. B. nicht robust Extraktionsdaten, kleinere Schemaabweichungen, Prompt zu wenig stringent formuliert -> teilweise unstabile Output)sind vorhanden. | **75% der max. erreichbaren Punkte** |\n",
    "| **Code lauffähig und korrekt**                         | Der Code erfüllt die Anforderungen des Aspekts vollständig und liefert die erwarteten Ergebnisse ohne Fehler (z. B. korrekte Extraktion, vollständige Bounding Boxes, saubere Integration). | **100% der max. erreichbaren Punkte** |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e675da2c",
   "metadata": {},
   "source": [
    "### A0. Setup (0 Punkte)\n",
    "\n",
    "Führen Sie diese Zellen aus, nachdem Sie die `.env` Datei mit Ihren API-Keys in den Codespace gezogen haben.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1fb54f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3d0429f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup abgeschlossen.\n"
     ]
    }
   ],
   "source": [
    "# === A0. Setup (Bitte Schlüssel nur lokal eintragen, nicht ins Versionskontrollsystem hochladen) ===\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "deepinfra_api_key = os.getenv(\"DEEPINFRA_API_KEY\")\n",
    "\n",
    "# OpenAI-Client\n",
    "openai_client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# Deepinfra-Client (OpenAI-kompatible API)\n",
    "deepinfra_client = OpenAI(\n",
    "    api_key=deepinfra_api_key,\n",
    "    base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    ")\n",
    "\n",
    "print(\"Setup abgeschlossen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f057e0",
   "metadata": {},
   "source": [
    "### A1. Funktion `ask_openai` (10 Punkte)\n",
    "\n",
    "Implementieren Sie eine Funktion `ask_openai`, die eine Anfrage mit Kontext an das OpenAI-Chat-API schickt und die Antwort als String zurückgibt.\n",
    "\n",
    "**Anforderungen:**\n",
    "1. Signatur:\n",
    "   ```python\n",
    "   def ask_openai(user_input: str, context: str) -> str:\n",
    "       ...\n",
    "   ```\n",
    "2. Verwenden Sie den bereits definierten `openai_client` aus A0.\n",
    "3. Nutzen Sie das Modell `\"gpt5-mini\"`.\n",
    "4. Die Funktion soll **nur den Textinhalt** der Antwort zurückgeben (ohne zusätzliche Prints).\n",
    "5. Der Kontext soll in den Systemprompt (injected) werden.\n",
    "\n",
    "Testen Sie Ihre Funktion danach mit **zwei Aufrufen**:\n",
    "- einmal mit dem Prompt: *\"Schreibe ein Haiku über Rekursion in der Programmierung.\"*\n",
    "- einmal mit dem Prompt: *\"Erkläre in einfachen Worten, was der Parameter `temperature` bei LLMs macht.\"*\n",
    "\n",
    "Drucken Sie beide Antworten aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58a1cbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input1 = \"Schreibe ein Haiku über Rekursion in der Programmierung.\"\n",
    "context1 = \"\"\"\n",
    "Rekursion ist eine Technik in der Programmierung, bei der eine Funktion sich selbst aufruft, um ein Problem in kleinere Teilprobleme zu zerlegen. \n",
    "Typischerweise gibt es dabei eine sogenannte Basisbedingung, bei der die Funktion ohne weiteren Selbstaufruf endet. \n",
    "Dadurch lassen sich komplexe Strukturen wie Bäume oder verschachtelte Listen oft sehr elegant bearbeiten. \n",
    "Allerdings kann Rekursion auch zu Fehlern führen, wenn die Basisbedingung fehlt oder nie erreicht wird. \n",
    "In vielen Fällen gibt es sowohl eine rekursive als auch eine iterative Lösung, die sich in Verständlichkeit und Effizienz unterscheiden können.\n",
    "\"\"\"\n",
    "\n",
    "user_input2 = \"Erkläre in einfachen Worten, was der Parameter `temperature` bei LLMs macht.\"\n",
    "context2 = \"\"\"\n",
    "Der Parameter 'temperature' steuert bei vielen Sprachmodellen, wie kreativ oder deterministisch die Antworten sind. \n",
    "Bei einer niedrigen Temperatur (z.B. 0.1) wählt das Modell eher die wahrscheinlichsten Worte und verhält sich dadurch konservativ und vorhersehbar. \n",
    "Bei einer höheren Temperatur (z.B. 0.8 oder 1.0) werden auch unwahrscheinlichere Worte stärker berücksichtigt, was zu vielfältigeren und kreativeren Antworten führt. \n",
    "Zu hohe Werte können allerdings dazu führen, dass die Ausgabe unzusammenhängend oder unsinnig wird. \n",
    "Die Wahl einer passenden Temperatur hängt daher vom Anwendungsfall ab, etwa ob man eher präzise Fakten oder kreative Ideen haben möchte.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8b72287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_openai(user_input: str, context: str) -> str:\n",
    "    completion = openai_client.chat.completions.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": context},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_input\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6ceaaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Funktion ruft sich\n",
      "Zerlegt das Problem weiter\n",
      "Bis die Basis ruft\n",
      "\n",
      "\n",
      "Kurz und einfach: Die Temperatur steuert, wie „zufällig“ oder „sicher“ ein Sprachmodell bei der Wortwahl ist.\n",
      "\n",
      "- Niedrige Temperatur (z.B. 0.0–0.3): das Modell wählt meist die wahrscheinlichsten Wörter — die Antworten sind vorhersehbar, präzise und konservativ.  \n",
      "- Mittlere Temperatur (z.B. 0.4–0.7): ein Kompromiss aus Verlässlichkeit und etwas Vielfalt.  \n",
      "- Hohe Temperatur (z.B. 0.8–1.0+): das Modell probiert auch weniger wahrscheinliche Wörter — die Ausgaben werden kreativer, aber auch unvorhersehbarer oder fehleranfälliger.\n",
      "\n",
      "Analog: Bei niedriger Temperatur wählt das Modell immer das „sicherste“ Wort, bei hoher Temperatur zieht es öfter ungewöhnliche Ideen aus dem Hut. Für Faktenantworten niedrig wählen, für kreative Texte höher.\n"
     ]
    }
   ],
   "source": [
    "# the two executions\n",
    "print(ask_openai(user_input1, context1)+\"\\n\\n\")\n",
    "print(ask_openai(user_input2, context2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60a505f",
   "metadata": {},
   "source": [
    "### A2. Deepinfra-Aufruf korrigieren und erweitern (10 Punkte)\n",
    "\n",
    "Unten finden Sie einen (bewusst) fehlerhaften Beispielaufruf an ein Modell über die Deepinfra-API.  \n",
    "**Ihre Aufgaben:**\n",
    "\n",
    "1. **Korrigieren** Sie den Aufruf so, dass er syntaktisch korrekt ist und mit dem `deepinfra_client` funktioniert.  \n",
    "   - Achten Sie insbesondere auf Parameter-Namen und den Zugriff auf das Antwortobjekt.\n",
    "2. Erweitern Sie den Code so, dass **zwei Gesprächsrunden** mit dem Modell stattfinden:\n",
    "   - Erste User-Frage: *\"Nenne zwei unterschiedliche Möglichkeiten, wie Jugendliche sich für den Umweltschutz politisch engagieren können.\"*\n",
    "   - Zweite User-Frage: *\"Erläutere die zweite Möglichkeit etwas genauer.\"*\n",
    "   Verwenden Sie dafür die gleiche `messages`-Liste und fügen Sie nur die neue Nutzer-Nachricht und die Assistenten-Antwort hinzu.\n",
    "3. Implementieren Sie eine einfache **Längenbegrenzung**:  \n",
    "   Wenn die endgültige Antwort-Variable mehr als **400 Zeichen** enthält, kürzen Sie sie auf 400 Zeichen und hängen Sie `\"...\"` an.\n",
    "\n",
    "Verwenden Sie ein geeignetes deutschsprachiges oder multilinguales Modell aus Deepinfra (z.B. ein Llama- oder Mixtral-Chatmodell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "734b8844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die zweite Möglichkeit, sich für den Umweltschutz politisch zu engagieren, besteht darin, dass Jugendliche sich in politischen Parteien oder Initiativen engagieren, die sich für den Umweltschutz einsetzen.\n",
      "\n",
      "**Politische Parteien:**\n",
      "\n",
      "*   Jugendliche können Mitglieder von Parteien wie Bündnis 90/Die Grünen werden, die sich stark für den Umweltschutz einsetzen.\n",
      "*   Sie können an Parteitagen teilnehmen, sich in Arbeitsgruppen engagieren und ihre Meinung zu umweltpolitischen Themen äußern.\n",
      "*   Durch die Mitarbeit in einer Partei können Jugendliche Einfluss auf die politische Agenda nehmen und sich für umweltfreundliche Politiken einsetzen.\n",
      "\n",
      "**Initiativen:**\n",
      "\n",
      "*   Jugendliche können sich auch in lokalen Initiativen wie Klimagruppen, Umweltverbänden oder Bürgerinitiativen engagieren.\n",
      "*   Diese Initiativen setzen sich oft für spezifische Umweltziele ein, wie z.B. die Reduzierung von Plastikmüll oder die Förderung erneuerbarer Energien.\n",
      "*   Durch die Mitarbeit in einer Initiative können Jugendliche ihre Stimme erheben und auf lokaler Ebene Veränderungen herbeiführen.\n",
      "\n",
      "Insgesamt bietet die Mitarbeit in politischen Parteien oder Initiativen Jugendlichen die Chance, ihre politischen Interessen zu vertreten und positive Veränderungen herbeizuführen.\n",
      "\n",
      "\n",
      "Die zweite Möglichkeit, sich für den Umweltschutz politisch zu engagieren, besteht darin, dass Jugendliche sich in politischen Parteien oder Initiativen engagieren, die sich für den Umweltschutz einsetzen.\n",
      "\n",
      "**Politische Parteien:**\n",
      "\n",
      "*   Jugendliche können Mitglieder von Parteien wie Bündnis 90/Die Grünen werden, die sich stark für den Umweltschutz einsetzen.\n",
      "*   Sie können an Parteitagen teilnehme...\n"
     ]
    }
   ],
   "source": [
    "# A2. Deepinfra-Aufruf – fehlerhafte Ausgangsversion\n",
    "# TODO (A2): Korrigieren und erweitern Sie diesen Code.\n",
    "\n",
    "model_name = \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\"  # Beispiel – ggf. anpassen\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Du bist eine sachliche, gut informierte Assistenz.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Nenne zwei unterschiedliche Möglichkeiten, wie Jugendliche sich für den Umweltschutz politisch engagieren können.\"}\n",
    "]\n",
    "\n",
    "chat_completion = deepinfra_client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "answer1 = {\"role\": \"assistant\", \"content\": chat_completion.choices[0].message.content}\n",
    "messages.append(answer1)\n",
    "question2 = {\"role\": \"user\", \"content\": \"Erläutere die zweite Möglichkeit etwas genauer.\"}\n",
    "messages.append(question2)\n",
    "\n",
    "chat_completion2 = deepinfra_client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    temperature=0.8\n",
    ")\n",
    "answer2 = chat_completion2.choices[0].message.content\n",
    "print(answer2+\"\\n\\n\")\n",
    "\n",
    "a2_short = (answer2[:400] + '...') if len(answer2) > 400 else answer2\n",
    "print(a2_short)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22afc978",
   "metadata": {},
   "source": [
    "### A3. Kurzfrage: Unterschiedliche Provider (5 Punkte)\n",
    "\n",
    "Beantworten Sie die folgende Frage in **3–4 Sätzen** in der Markdown-Zelle darunter:\n",
    "\n",
    "> Nennen Sie **einen Vorteil** der Nutzung eines gehosteten Dienstes wie OpenAI im Vergleich zu einem selbst gehosteten Modell,\n",
    "> und nennen Sie **mindestens einen Nachteil bzw. ein Risiko**.\n",
    "\n",
    "Gehen Sie dabei kurz auf Aspekte wie zum Beispiel Leistung, Kosten, Flexibilität oder Datenschutz ein."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a6406e",
   "metadata": {},
   "source": [
    "**Antwort A3:**\n",
    "Gehostete Dienste erlauben den Zugriff auf grundsätzlich \"bessere\" Modelle (Foundation Models wie GPT5.1), die mit sehr teurer Hardware trainiert wurden und auf teurer Hardware laufen. Diese sind lokal gar nicht oder nur begrenzt (z.B. GPT OSS) verfügbar und selbst wenn, wäre die benötigte Hardware dafür sehr teuer. Sie sind ausserdem meist stabiler verfügbar (bei den Hostern gibt es Ausfallsicherheit, SLAs, ...)\n",
    "\n",
    "Grösstes Risiko / Problem davon ist Datenschutz. Es kann nicht begrenzt werden wohin die Daten fliessen und maximal vertraglich festgelegt werden, wer Zugriff auf diese Daten hat bzw. wie diese verarbeitet oder zum Lernen der Modelle weiterverwendet werden. Ausserdem ist die Gefahr für Lock-In Risiken gross und bei closed-source / closed-parameter Modellen weiss man zudem nicht, wie sie gelernt haben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a8117f",
   "metadata": {},
   "source": [
    "---\n",
    "## Teil B – Retrieval-Augmented Generation (RAG) (35 Punkte)\n",
    "\n",
    "In diesem Teil arbeiten Sie mit einer kleinen RAG-Pipeline.  \n",
    "Die zugrundeliegenden Texte behandeln das Thema **\"Jugend, Politik, Umwelt & Partizipation\"** in deutscher Sprache.\n",
    "\n",
    "Die folgenden Objekte werden in vorbereiteten Zellen erstellt:\n",
    "- `documents`: Liste von Rohtexten (z.B. aus PDF- oder Textdateien)\n",
    "- `chunks`: Liste von Textchunks\n",
    "- `embedder`: Objekt mit einer Methode `.encode(texts, convert_to_numpy=True)`\n",
    "- `index`: ein FAISS-Index, der Embeddings von `chunks` enthält\n",
    "\n",
    "Sie sollen nun die fehlenden Teile der Pipeline implementieren bzw. anpassen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3ad34c",
   "metadata": {},
   "source": [
    "### B0. RAG-Setup (0 Punkte – bitte nicht verändern)\n",
    "\n",
    "Diese Zelle lädt die Dokumente, erzeugt Chunks, berechnet Embeddings und baut den FAISS-Index auf.  \n",
    "**Verändern Sie diese Zelle nicht.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b934c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === B0. RAG-Setup (Bitte nicht verändern) ===\n",
    "import os\n",
    "from typing import List\n",
    "import glob\n",
    "from PyPDF2 import PdfReader\n",
    "import numpy as np\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# Dokumente Laden\n",
    "\n",
    "DATA_DIR = \"data/*.pdf\"  # Ordner mit Texten zu \"Jugend, Politik, Umwelt & Partizipation\"\n",
    "\n",
    "text = \"\"\n",
    "\n",
    "for pdf_path in glob.glob(DATA_DIR):\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        reader = PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += \" \" + page_text\n",
    "\n",
    "# Split into chunks\n",
    "\n",
    "splitter_a = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2500,\n",
    "    chunk_overlap=250\n",
    ")\n",
    "\n",
    "chunks = splitter_a.split_text(text)\n",
    "\n",
    "print(f\"Anzahl Chunks: {len(chunks)}\")\n",
    "\n",
    "# 3. Embedder und Embeddings\n",
    "embedder = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "embeddings = embedder.encode(chunks, convert_to_numpy=True)\n",
    "\n",
    "# 4. FAISS-Index erstellen\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(embeddings)\n",
    "\n",
    "print(\"RAG-Setup abgeschlossen.\")\n",
    "\n",
    "\n",
    "os.makedirs(\"faiss\", exist_ok=True)\n",
    "faiss.write_index(index, \"faiss/faiss_exam.index\")\n",
    "with open(\"faiss/chunks_exam.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27faf433",
   "metadata": {},
   "source": [
    "### Run in case of crash later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde60375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = faiss.read_index(\"faiss/faiss_exam.index\")\n",
    "\n",
    "# with open(\"faiss/chunks_exam.pkl\", \"rb\") as f:\n",
    "#    chunks = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f321272d",
   "metadata": {},
   "source": [
    "### B1. Funktion `retrieve_texts` korrigieren (10 Punkte)\n",
    "\n",
    "Die folgende Funktion soll zu einer Anfrage (`query`) die **Top-`k` ähnlichsten Chunks** zurückgeben.\n",
    "Die Implementierung ist fehlerhaft.\n",
    "\n",
    "**Ihre Aufgaben:**\n",
    "1. Korrigieren Sie die Funktion so, dass sie:\n",
    "   - die Query korrekt mit dem `embedder` embedden kann,\n",
    "   - den FAISS-Index `index` korrekt abfragt und\n",
    "   - eine **Liste von Strings** mit den gefundenen Chunks zurückgibt.\n",
    "2. Achten Sie insbesondere auf die richtige Form der Embedding-Arrays und den Umgang mit den Indizes.\n",
    "\n",
    "Verwenden Sie die bereits existierenden Objekte `index`, `chunks` und `embedder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93522671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def retrieve_texts(query: str, k: int, index, chunks: List[str], embedder) -> List[str]:\n",
    "    \"\"\"Gibt die Top-k ähnlichsten Chunks für eine Query zurück.\n",
    "\n",
    "    TODO (B1): Funktion korrigieren.\n",
    "    \"\"\"\n",
    "    # FEHLERHAFTE BEISPIELIMPLEMENTIERUNG:\n",
    "    query_emb = embedder.encode(query, convert_to_numpy=True)  # Form kann problematisch sein\n",
    "    distances, indices = index.search(query_emb, k)  # hier kann ein Fehler auftreten\n",
    "    return [chunks[indices]]  # ebenfalls fehlerhaft\n",
    "\n",
    "# Sie können die Funktion z.B. mit einer Test-Query ausprobieren:\n",
    "test_query = \"Wie können Jugendliche sich politisch für den Klimaschutz engagieren?\"\n",
    "try:\n",
    "    test_results = retrieve_texts(test_query, k=3, index=index, chunks=chunks, embedder=embedder)\n",
    "    for i, r in enumerate(test_results, start=1):\n",
    "        print(f\"Chunk {i} (Ausschnitt):\", r[:200], \"\\n---\\n\")\n",
    "except Exception as e:\n",
    "    print(\"Fehler bei Testabfrage (erwartet, solange die Funktion noch nicht korrigiert ist):\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0251e5f3",
   "metadata": {},
   "source": [
    "### B2. RAG-Antwortfunktion `answer_query` anpassen (10 Punkte)\n",
    "\n",
    "Unten sehen Sie eine Funktion `answer_query`, die eine Nutzerfrage beantwortet, indem sie:\n",
    "1. mit `retrieve_texts` passende Chunks zum Thema findet,\n",
    "2. diese als Kontext an das LLM schickt und\n",
    "3. eine Antwort generiert.\n",
    "\n",
    "Die Funktion ist noch nicht korrekt auf das Thema **\"Jugend, Politik, Umwelt & Partizipation\"** und auf RAG zugeschnitten.\n",
    "\n",
    "**Ihre Aufgaben:**\n",
    "1. Passen Sie den `system_prompt` an die neue Domäne an und geben sie klare Anweisungen wie mit dem Kontext umzugehen ist.  \n",
    "3. Verwenden Sie das Modell `\"gpt-5-mini\"` über den `openai_client` aus Teil A.\n",
    "4. Wählen Sie einen sinnvollen Standardwert für `k`und kommentieren Sie in **einem kurzen Kommentar im Code**, warum diese Wahl für diesen Anwendungsfall plausibel ist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607ad253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query: str, k: int, index, chunks: List[str], embedder, client: OpenAI) -> str:\n",
    "    \"\"\"Beantwortet eine Nutzerfrage mittels RAG.\n",
    "\n",
    "    TODO (B2):\n",
    "    - Prompt anpassen\n",
    "    - Sicherstellen, dass nur Kontext verwendet wird\n",
    "    - openai_client und gpt-5-mini nutzen\n",
    "    \"\"\"\n",
    "    retrieved_chunks = retrieve_texts(query, k, index, chunks, embedder)\n",
    "    context = \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"Du bist eine hilfreiche Assistenz, die Fragen zu MEDIZINISCHEN LEITLINIEN beantwortet. \"\n",
    "        \"(TODO: Dies ist noch nicht an die neue Domäne angepasst.)\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Kontext:\\n{context}\\n\\nFrage: {query}\"},\n",
    "    ]\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content.strip()\n",
    "\n",
    "# Beispielaufruf (wenn B1 und B2 korrekt implementiert sind):\n",
    "example_query = \"Welche Formen politischer Beteiligung von Jugendlichen im Umweltschutz werden im Text genannt?\"\n",
    "try:\n",
    "    answer = answer_query(example_query, k=, index=index, chunks=chunks, embedder=embedder, client=openai_client)\n",
    "    print(answer)\n",
    "except Exception as e:\n",
    "    print(\"Fehler bei Beispielaufruf (erwartet, solange B1/B2 noch nicht fertig sind):\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fc5bb2",
   "metadata": {},
   "source": [
    "### B3. Einfache Retrieval-Sanity-Checks (5 Punkte)\n",
    "\n",
    "Erweitern Sie die Funktion `answer_query` aus B2 wie folgt:\n",
    "\n",
    "1. **Vor dem LLM-Aufruf** soll ein kurzer Ausschnitt (ca. die ersten 200 Zeichen) des **besten (Top-1) Chunks** ausgegeben werden, z.B. mit `print(\"Top-1 Chunk:\", ...)`.\n",
    "2. Falls aus irgendeinem Grund **keine Chunks** zurückgegeben werden (z.B. leere Liste), soll die Funktion **keinen LLM-Aufruf** machen, sondern einen verständlichen Hinweis-String zurückgeben, z.B.:\n",
    "   > \"Es konnten keine passenden Textstellen gefunden werden.\"\n",
    "\n",
    "Implementieren Sie diese Änderungen direkt in der Funktion `answer_query` oben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65034c5",
   "metadata": {},
   "source": [
    "_Hinweis: Sie müssen hier keine neue Zelle schreiben. Passen Sie die bestehende Implementierung von `answer_query` direkt an._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87068cb8",
   "metadata": {},
   "source": [
    "### B4. Fragen und Kriterien für eine Evaluation des RAG-Systems (10 Punkte)\n",
    "\n",
    "Sie haben in diesem Notebook ein RAG-System aufgebaut, das Fragen zum Thema\n",
    "**„Jugend, Politik, Umwelt & Partizipation“** beantwortet.\n",
    "\n",
    "In dieser Aufgabe sollen Sie das LLM nutzen, um:\n",
    "\n",
    "1. sinnvolle **Evaluationsfragen** zu generieren, mit denen man das System testen könnte, und\n",
    "2. **Kriterien** vorschlagen zu lassen, auf die man bei der Evaluation achten sollte.\n",
    "\n",
    "---\n",
    "\n",
    "#### B4.1 – Funktion `generate_eval_questions` implementieren\n",
    "\n",
    "Implementieren Sie eine Funktion, die mit Hilfe des OpenAI-Clients _**10**_ Evaluationsfragen generiert.\n",
    "\n",
    "**Anforderungen:**\n",
    "\n",
    "1. Signatur:\n",
    "   ```python\n",
    "   def generate_eval_questions(chunks, num_chunks: int = 5) -> str:\n",
    "       ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b31dc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_eval_questions(chunks, num_chunks) -> list:\n",
    "\n",
    "    # B4.2 – Kriterien für die Auswertung generieren\n",
    "\n",
    "    # TODO (B4.2):\n",
    "    # - eigenen Systemprompt definieren (Evaluationsexpert*in) \n",
    "    # - jeden chunk aus dem sample in den prompt injecten\n",
    "    # - jede generierte Frage in die Liste einfügen\n",
    "\n",
    "    questions = []\n",
    "    prompt_eval = (\n",
    "        \"TODO: Formulieren Sie hier eine Anfrage, in der Sie das Modell bitten, \"\n",
    "        \"Fragen zu genierieren, die zur Evaluation eines \"\n",
    "        \"RAG-Systems (Jugend, Politik, Umwelt & Partizipation) achten sollte. \" \n",
    "        \"Es soll genau eine Frage, und nur die Frage ausgegeben werden\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502ea877",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = generate_eval_questions()\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95190cd",
   "metadata": {},
   "source": [
    "---\n",
    "## Ende der Prüfung\n",
    "\n",
    "Überprüfen Sie, ob alle Code-Zellen ausgeführt wurden und alle Antworten (auch die theoretischen Fragen) eingetragen sind.\n",
    "\n",
    "Viel Erfolg!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c2b83b",
   "metadata": {},
   "source": [
    "### Jupyter notebook --footer info-- (please always provide this at the end of each notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdb9503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import socket\n",
    "from platform import python_version\n",
    "from datetime import datetime\n",
    "\n",
    "print('-----------------------------------')\n",
    "print(os.name.upper())\n",
    "print(platform.system(), '|', platform.release())\n",
    "print('Datetime:', datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print('Python Version:', python_version())\n",
    "print('IP Address:', socket.gethostbyname(socket.gethostname()))\n",
    "print('-----------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
